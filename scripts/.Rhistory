p_M1 = NA,
p_M2 = NA,
M2_a = NA,
M2_b = NA,
prob_T_predicted = NA)
prior_fair <- .25
iter.df <- rbind(data.frame(iter= 0, obs = NA,
margLik_M1 = NA,
margLik_M2 = NA,
p_M1 = prior_fair,
p_M2 = 1-prior_fair,
M2_a = 1, # prior belief on shape of alpha in model 2
M2_b = 1,
prob_T_predicted = 0.5),
iter.df) # prior belief on shape of beta in model 2
for(i in 1:n_obs){
print(i)
iter.df$obs[i+1] <- obs <- rbinom(n = 1, size = n, prob = w_tru) # data observed
M2_a <- iter.df$M2_a[i]
M2_b <- iter.df$M2_b[i]
# Marginal likelihood of M1 (fair)
iter.df$margLik_M1[i + 1] <- dbinom(x = obs, size = n, prob = 0.5)
# Marginal Likelihood of M2
iter.df$margLik_M2[i + 1] <- integrate(function(x){
return(dbinom(obs, size = n, prob = x)*dbeta(x = x, shape1 = M2_a, shape2 = M2_b))
},
lower = 0, upper = 1)$value
# Posterior probability unnormalized
p_M1_unnrmlz <- iter.df$margLik_M1[i + 1]*iter.df$p_M1[i]
p_M2_unnrmlz <- iter.df$margLik_M2[i + 1]*iter.df$p_M2[i]
# Normalize the posterior probabilities (only two models)
iter.df$p_M1[i+1] <- p_M1_unnrmlz/(p_M1_unnrmlz + p_M2_unnrmlz)
iter.df$p_M2[i+1] <- p_M2_unnrmlz/(p_M1_unnrmlz + p_M2_unnrmlz)
# update the beta
iter.df$M2_a[i+1] <- M2_a + obs
iter.df$M2_b[i+1] <- n - obs + M2_b
# averaged prob of heads
iter.df$prob_T_predicted[i+1] <- iter.df$p_M1[i+1]*0.5 +
iter.df$p_M2[i+1]*(iter.df$M2_a[i+1]/(iter.df$M2_a[i+1] + iter.df$M2_b[i+1]))
}
plot(x = iter.df$iter,iter.df$p_M1, type = "l",ylim = c(0,1), col = "blue")
points(x = iter.df$iter,iter.df$p_M2, type = "l",col = "red")
points(iter.df$iter, y = iter.df$prob_T_predicted, type = "l", col = "black")
w_tru <- .5
n <- 1 # numober of trials per "experiment"
n_obs <- 2000# number of experiments
iter.df <-data.frame(iter = 1:n_obs,
obs = NA,
margLik_M1 = NA,
margLik_M2 = NA,
p_M1 = NA,
p_M2 = NA,
M2_a = NA,
M2_b = NA,
prob_T_predicted = NA)
prior_fair <- .25
iter.df <- rbind(data.frame(iter= 0, obs = NA,
margLik_M1 = NA,
margLik_M2 = NA,
p_M1 = prior_fair,
p_M2 = 1-prior_fair,
M2_a = 1, # prior belief on shape of alpha in model 2
M2_b = 1,
prob_T_predicted = 0.5),
iter.df) # prior belief on shape of beta in model 2
for(i in 1:n_obs){
print(i)
iter.df$obs[i+1] <- obs <- rbinom(n = 1, size = n, prob = w_tru) # data observed
M2_a <- iter.df$M2_a[i]
M2_b <- iter.df$M2_b[i]
# Marginal likelihood of M1 (fair)
iter.df$margLik_M1[i + 1] <- dbinom(x = obs, size = n, prob = 0.5)
# Marginal Likelihood of M2
iter.df$margLik_M2[i + 1] <- integrate(function(x){
return(dbinom(obs, size = n, prob = x)*dbeta(x = x, shape1 = M2_a, shape2 = M2_b))
},
lower = 0, upper = 1)$value
# Posterior probability unnormalized
p_M1_unnrmlz <- iter.df$margLik_M1[i + 1]*iter.df$p_M1[i]
p_M2_unnrmlz <- iter.df$margLik_M2[i + 1]*iter.df$p_M2[i]
# Normalize the posterior probabilities (only two models)
iter.df$p_M1[i+1] <- p_M1_unnrmlz/(p_M1_unnrmlz + p_M2_unnrmlz)
iter.df$p_M2[i+1] <- p_M2_unnrmlz/(p_M1_unnrmlz + p_M2_unnrmlz)
# update the beta
iter.df$M2_a[i+1] <- M2_a + obs
iter.df$M2_b[i+1] <- n - obs + M2_b
# averaged prob of heads
iter.df$prob_T_predicted[i+1] <- iter.df$p_M1[i+1]*0.5 +
iter.df$p_M2[i+1]*(iter.df$M2_a[i+1]/(iter.df$M2_a[i+1] + iter.df$M2_b[i+1]))
}
plot(x = iter.df$iter,iter.df$p_M1, type = "l",ylim = c(0,1), col = "blue")
points(x = iter.df$iter,iter.df$p_M2, type = "l",col = "red")
points(iter.df$iter, y = iter.df$prob_T_predicted, type = "l", col = "black")
tail(iter.df)
curve(dbeta(x, .5,.5))
curve(dbeta(x, .75,.75))
curve(dbeta(x, .75,.75),ylim = c(0,NA))
curve(dbeta(x, .75,.75),ylim = c(0,10))
curve(dbeta(x, .5,.5),ylim = c(0,10))
curve(dbeta(x, .25,.25),ylim = c(0,10))
w_tru <- .5
n <- 1 # numober of trials per "experiment"
n_obs <- 2000# number of experiments
iter.df <-data.frame(iter = 1:n_obs,
obs = NA,
margLik_M1 = NA,
margLik_M2 = NA,
p_M1 = NA,
p_M2 = NA,
M2_a = NA,
M2_b = NA,
prob_T_predicted = NA)
prior_fair <- .25
iter.df <- rbind(data.frame(iter= 0, obs = NA,
margLik_M1 = NA,
margLik_M2 = NA,
p_M1 = prior_fair,
p_M2 = 1-prior_fair,
M2_a = .5, # prior belief on shape of alpha in model 2
M2_b = .5,
prob_T_predicted = 0.5),
iter.df) # prior belief on shape of beta in model 2
for(i in 1:n_obs){
print(i)
iter.df$obs[i+1] <- obs <- rbinom(n = 1, size = n, prob = w_tru) # data observed
M2_a <- iter.df$M2_a[i]
M2_b <- iter.df$M2_b[i]
# Marginal likelihood of M1 (fair)
iter.df$margLik_M1[i + 1] <- dbinom(x = obs, size = n, prob = 0.5)
# Marginal Likelihood of M2
iter.df$margLik_M2[i + 1] <- integrate(function(x){
return(dbinom(obs, size = n, prob = x)*dbeta(x = x, shape1 = M2_a, shape2 = M2_b))
},
lower = 0, upper = 1)$value
# Posterior probability unnormalized
p_M1_unnrmlz <- iter.df$margLik_M1[i + 1]*iter.df$p_M1[i]
p_M2_unnrmlz <- iter.df$margLik_M2[i + 1]*iter.df$p_M2[i]
# Normalize the posterior probabilities (only two models)
iter.df$p_M1[i+1] <- p_M1_unnrmlz/(p_M1_unnrmlz + p_M2_unnrmlz)
iter.df$p_M2[i+1] <- p_M2_unnrmlz/(p_M1_unnrmlz + p_M2_unnrmlz)
# update the beta
iter.df$M2_a[i+1] <- M2_a + obs
iter.df$M2_b[i+1] <- n - obs + M2_b
# averaged prob of heads
iter.df$prob_T_predicted[i+1] <- iter.df$p_M1[i+1]*0.5 +
iter.df$p_M2[i+1]*(iter.df$M2_a[i+1]/(iter.df$M2_a[i+1] + iter.df$M2_b[i+1]))
}
plot(x = iter.df$iter,iter.df$p_M1, type = "l",ylim = c(0,1), col = "blue")
points(x = iter.df$iter,iter.df$p_M2, type = "l",col = "red")
points(iter.df$iter, y = iter.df$prob_T_predicted, type = "l", col = "black")
w_tru <- .5
n <- 1 # numober of trials per "experiment"
n_obs <- 200# number of experiments
iter.df <-data.frame(iter = 1:n_obs,
obs = NA,
margLik_M1 = NA,
margLik_M2 = NA,
p_M1 = NA,
p_M2 = NA,
M2_a = NA,
M2_b = NA,
prob_T_predicted = NA)
prior_fair <- .25
iter.df <- rbind(data.frame(iter= 0, obs = NA,
margLik_M1 = NA,
margLik_M2 = NA,
p_M1 = prior_fair,
p_M2 = 1-prior_fair,
M2_a = .5, # prior belief on shape of alpha in model 2
M2_b = .5,
prob_T_predicted = 0.5),
iter.df) # prior belief on shape of beta in model 2
for(i in 1:n_obs){
print(i)
iter.df$obs[i+1] <- obs <- rbinom(n = 1, size = n, prob = w_tru) # data observed
M2_a <- iter.df$M2_a[i]
M2_b <- iter.df$M2_b[i]
# Marginal likelihood of M1 (fair)
iter.df$margLik_M1[i + 1] <- dbinom(x = obs, size = n, prob = 0.5)
# Marginal Likelihood of M2
iter.df$margLik_M2[i + 1] <- integrate(function(x){
return(dbinom(obs, size = n, prob = x)*dbeta(x = x, shape1 = M2_a, shape2 = M2_b))
},
lower = 0, upper = 1)$value
# Posterior probability unnormalized
p_M1_unnrmlz <- iter.df$margLik_M1[i + 1]*iter.df$p_M1[i]
p_M2_unnrmlz <- iter.df$margLik_M2[i + 1]*iter.df$p_M2[i]
# Normalize the posterior probabilities (only two models)
iter.df$p_M1[i+1] <- p_M1_unnrmlz/(p_M1_unnrmlz + p_M2_unnrmlz)
iter.df$p_M2[i+1] <- p_M2_unnrmlz/(p_M1_unnrmlz + p_M2_unnrmlz)
# update the beta
iter.df$M2_a[i+1] <- M2_a + obs
iter.df$M2_b[i+1] <- n - obs + M2_b
# averaged prob of heads
iter.df$prob_T_predicted[i+1] <- iter.df$p_M1[i+1]*0.5 +
iter.df$p_M2[i+1]*(iter.df$M2_a[i+1]/(iter.df$M2_a[i+1] + iter.df$M2_b[i+1]))
}
plot(x = iter.df$iter,iter.df$p_M1, type = "l",ylim = c(0,1), col = "blue")
points(x = iter.df$iter,iter.df$p_M2, type = "l",col = "red")
points(iter.df$iter, y = iter.df$prob_T_predicted, type = "l", col = "black")
source("~/ProgramFiles/Github/NASA_ForestScaling/LifeHistory/forestGEO/scripts/STAN_exampleAnalysis.R")
source("~/ProgramFiles/Github/NASA_ForestScaling/LifeHistory/forestGEO/scripts/STAN_exampleAnalysis.R")
# location of the data
loc_data <- paste0("G:/Shared drives/NASA_ForestScaling/",
"LifeHistoryStrats/",
"data/ForestGEO/processed/")
# load in the data
load(paste0(loc_data,"exampleData.R"))
# Packages
invisible(lapply(list("tidyverse",
"rstan",
"parallel"),
FUN = require,
character.only = T))
# some Rstan options I like to use
rstan_options(auto_write = TRUE,  #   whether to write the compiled C++ script
# prevents needing to recompile each time (?)
javascript = FALSE) #   prevents using the github C++ compiler
# on TRUE, will use the internet to reach out
# to the github server which has caused me
# issues before
# Default vals for runs (override individually if desired.)
warm_its <- 2000  # number of warmups
smpl_its <- 5000  # number of USED iter, (MCMC will run for smpl + warm)
n_coreuse <- 5    # number of cores to use
n_chains <- 5     # number of chains to run
n_thin <- 1       # thinning, 1 = none
NUTS_adapt_delta <- 0.85 # adapt delta, default is 0.801, larger means
# ~smaller "steps" so less divergence, but less
# efficient
# Prep Data for stan
# Growth data
exmpl_grow.L <- list(
# number of stems, an integer
n_stem = nrow(exmpl_grow.df),
# number of species, an integer
n_spp = length(unique(exmpl_grow.df$species)),
# vector of the species integers
species = exmpl_grow.df$species,
# vector of growth values (log transformed)
loggrowth_vals = exmpl_grow.df$grow_val)
# Survival/binary data
exmpl_surv.L <- list(
# number of stems, an integer
n_stem = nrow(exmpl_surv.df),
# number of species, an integer
n_spp = length(unique(exmpl_surv.df$species)),
# vector of the species integers
species = exmpl_surv.df$species,
# vector of binary survival values
bindat = exmpl_surv.df$survival,
# Length of time between observations
t_interv = exmpl_surv.df$t)
# --- Run the models ----------------------------------------------------------
### Growth Model ###
t_i <- Sys.time()
list.files
list.files()
getwd()
source("~/ProgramFiles/Github/NASA_ForestScaling/LifeHistory/forestGEO/scripts/STAN_exampleAnalysis.R")
source("~/ProgramFiles/Github/NASA_ForestScaling/LifeHistory/forestGEO/scripts/STAN_exampleAnalysis.R")
# location of the data
loc_data <- paste0("G:/Shared drives/NASA_ForestScaling/",
"LifeHistoryStrats/",
"data/ForestGEO/processed/")
# load in the data
load(paste0(loc_data,"exampleData.R"))
# Packages
invisible(lapply(list("tidyverse",
"rstan",
"parallel"),
FUN = require,
character.only = T))
# some Rstan options I like to use
rstan_options(auto_write = TRUE,  # whether to write the compiled C++ script
#   prevents recompiling each time (?)
javascript = FALSE) # prevents using the github C++ compiler
#   on TRUE, will use the internet to access
#   to the github server which has caused me
#   issues before
# Default vals for runs (override individually if desired.)
warm_its <- 2000  # number of warmups
smpl_its <- 5000  # number of USED iter, (MCMC will run for smpl + warm)
n_coreuse <- 5    # number of cores to use
n_chains <- 5     # number of chains to run
n_thin <- 1       # thinning, 1 = none
NUTS_adapt_delta <- 0.85 # adapt delta, default is 0.801, larger means
# ~smaller "steps" so less divergence, but less
# efficient
# Prep Data for stan
# Growth data
exmpl_grow.L <- list(
# number of stems, an integer
n_stem = nrow(exmpl_grow.df),
# number of species, an integer
n_spp = length(unique(exmpl_grow.df$species)),
# vector of the species integers
species = exmpl_grow.df$species,
# vector of growth values (log transformed)
loggrowth_vals = exmpl_grow.df$grow_val)
# Survival/binary data
exmpl_surv.L <- list(
# number of stems, an integer
n_stem = nrow(exmpl_surv.df),
# number of species, an integer
n_spp = length(unique(exmpl_surv.df$species)),
# vector of the species integers
species = exmpl_surv.df$species,
# vector of binary survival values
bindat = exmpl_surv.df$survival,
# Length of time between observations
t_interv = exmpl_surv.df$t)
# --- Run the models ----------------------------------------------------------
### Growth Model ###
t_i <- Sys.time()
# Run the fit in STAN
fit_grow <- stan(file = "./STAN/growth_lognormal_estimate.stan",
"exmplGrowth",
data = exmpl_grow.L,
warmup = warm_its,          # burn-in
iter = smpl_its + warm_its, # number of iterations (total)
chains = n_chains,
cores = n_coreuse,
thin = n_thin,
control = list(adapt_delta = NUTS_adapt_delta))
devtools::install_github("rmcelreath/rethinking")
rethinking::rlkjcorr(1,K = 2)
rethinking::rlkjcorr(1,K = 2)
rethinking::rlkjcorr(1,K = 2)
rethinking::rlkjcorr(1,K = 2)
rethinking::rlkjcorr(1,K = 2)
rethinking::rlkjcorr(1,K = 2)
rethinking::rlkjcorr(1,K = 2)
rethinking::rlkjcorr(1,K = 2)
rethinking::rlkjcorr(1,K = 5)
rethinking::rlkjcorr(1,K = 5) %>% apply(., 1, FUN = sum)
rethinking::rlkjcorr(1,K = 5) |> apply( 1, FUN = sum)
rethinking::rlkjcorr(1,K = 5) |> apply( 1, FUN = function(x)return(sum(x^2)))
rethinking::rlkjcorr(1,K = 5, eta = 5)
rethinking::rlkjcorr(1,K = 5, eta = 5)
rethinking::rlkjcorr(1,K = 5, eta = 5)
rethinking::rlkjcorr(1,K = 5) |> apply( 1, FUN = function(x)return(sum(x^2)))
rethinking::rlkjcorr(1,K = 5) |> apply( 1, FUN = function(x)return(sum(x)^2))
install.packages("Rditools")
install.packages("Rdimtools")
library(Rdimtools)
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])
out1  <- do.bpca(X, ndim=2)
plot(out1$Y)
ou1 %>% View
library(tidyverse)
ou1 %>% View
out1 %>% View
library(devtools)
install_github("bPCA", username="petrkeil")
devtools::install_github("petrkeil/bPCA")
?do.bpca
out1  <- do.bpca(X, ndim=2)
out1$Y
data(iris)
set.seed(100)
subid = sample(1:150,50)
X     = as.matrix(iris[subid,1:4])
lab   = as.factor(iris[subid,5])
X
out1  <- do.bpca(X, ndim=1)
out1  <- do.bpca(X, ndim=2)
plot(out1$Y, col=lab, pch=19, cex=0.8, main="Bayesian PCA")
out1$Y
out1  <- do.bpca(X, ndim=5)
out1  <- do.bpca(X, ndim=4)
out1  <- do.bpca(X, ndim=3)
list.files()
inf.df <- read.csv("infection_data.csv")
getwd()
order(c(0,1))
sort(c(0,1))
sort(c(0,1),decreasing = T)
d.df <- data.frame(A = rep(c(1,0),4), B = c(rep(1,1,0,0),2), R = sort(rep(c(0,1),4),decreasing = T))
d.df
d.df <- data.frame(A = rep(c(1,0),4), B = rep(c(1,1,0,0)),2), R = sort(rep(c(0,1),4),decreasing = T))
d.df <- data.frame(A = rep(c(1,0),4), B = rep(c(1,1,0,0),2), R = sort(rep(c(0,1),4),decreasing = T))
d.df
d.df <- data.frame(A = rep(c(1,0),4),
B = rep(c(1,1,0,0),2),
R = sort(rep(c(0,1),4), decreasing = T),
ppn = c(.012,.0013,.00287,.317,0.003,.00033,.00591,.654))
d.df$ppn %>% sum
d.df$ppn |> sum
d.df$ppn |> sum()
d.df$ppn <- (d.df$ppn*1000)/1000
d.df
d.df <- data.frame(A = rep(c(1,0),4),
B = rep(c(1,1,0,0),2),
R = sort(rep(c(0,1),4), decreasing = T),
ppn = c(.012,.0013,.00287,.317,0.003,.00033,.00591,.654))
library(tidyverse)
n <- 5000
n <- 5000
d.df <- data.frame(A = rep(c(1,0),4),
B = rep(c(1,1,0,0),2),
R = sort(rep(c(0,1),4), decreasing = T),
ppn = c(.012,.0013,.00287,.317,0.003,.00033,.00591,.654))
d.df <- d.df %>% mutate(., ppn =round(n*ppn))
d.df
d.df <- d.df %>% mutate(., ppn =round(n*ppn)) %>% mutate(ppn = ppn/sum(ppn))
n <- 5000
d.df <- data.frame(A = rep(c(1,0),4),
B = rep(c(1,1,0,0),2),
R = sort(rep(c(0,1),4), decreasing = T),
ppn = c(.012,.0013,.00287,.317,0.003,.00033,.00591,.654))
d.df <- d.df %>% mutate(., ppn =round(n*ppn)) %>% mutate(ppn = ppn/sum(ppn))
d.df
d.df %>% filter(as.logical(B)) %>% pull(ppn) %>% sum %>% /R
?/
?'/'
B_gR <- d.df %>% filter(as.logical(B)) %>% pull(ppn) %>% sum %>% `/`R
B_gR <- d.df %>% filter(as.logical(B)) %>% pull(ppn) %>% sum(./R)
d.df <- d.df %>% mutate(., ppn =round(n*ppn)) %>% mutate(ppn = ppn/sum(ppn))
A <- d.df %>% filter(as.logical(A)) %>% pull(ppn) %>% sum
B <- d.df %>% filter(as.logical(B)) %>% pull(ppn) %>% sum
R <- d.df %>% filter(as.logical(R)) %>% pull(ppn) %>% sum
B_gR <- d.df %>% filter(as.logical(B)) %>% pull(ppn) %>% sum(./R)
B_gR
B
A_gB <- d.df %>% filter(as.logical(A)) %>% pull(ppn) %>% sum(./B)
A_gB
d.df %>% filter(as.logical(A)) %>% pull(ppn) %>% sum(./B)
B
0*0
d.df %>% filter(as.logical(B*R)) %>% pull(ppn) %>% sum(./R)
B_gR <- d.df %>% filter(as.logical(B*R)) %>% pull(ppn) %>% sum(./R)
B
d.df %>% filter(as.logical(A*B)) %>% pull(ppn) %>% sum(./B)
B
BnR <-  d.df %>% filter(as.logical(R*B)) %>% pull(ppn) %>% sum
d.df %>% filter(as.logical(A*B*R)) %>% pull(ppn) %>% sum(./BnR)
d.df %>% filter(as.logical(B*R)) %>% pull(ppn) %>% sum(./R)
d.df %>% filter(as.logical(B)) %>% pull(ppn) %>% sum
A_gR <- d.df %>% filter(as.logical(A*R)) %>% pull(ppn) %>% sum(./R)
A_gR
A
d.df %>% filter(as.logical(A*B*R)) %>% pull(ppn) %>% sum(./BnR)
A
d.df %>% filter(as.logical(A*B*R)) %>% pull(ppn) %>% sum(./BnR)
d.df %>% filter(as.logical(A*B)) %>% pull(ppn) %>% sum(./B)
d.df <- d.df %>% mutate(., ppn =round(n*ppn)) %>% mutate(ppn = ppn/sum(ppn))
A <- d.df %>% filter(as.logical(A)) %>% pull(ppn) %>% sum
B <- d.df %>% filter(as.logical(B)) %>% pull(ppn) %>% sum
R <- d.df %>% filter(as.logical(R)) %>% pull(ppn) %>% sum
BnR <- d.df %>% filter(as.logical(R*B)) %>% pull(ppn) %>% sum
AnB <- d.df %>% filter(as.logical(A*B)) %>% pull(ppn) %>% sum
AnR <- d.df %>% filter(as.logical(A*R)) %>% pull(ppn) %>% sum
B_gR <- d.df %>% filter(as.logical(B*R)) %>% pull(ppn) %>% sum(./R)
A_gB <- d.df %>% filter(as.logical(A*B)) %>% pull(ppn) %>% sum(./B)
A_gR <- d.df %>% filter(as.logical(A*R)) %>% pull(ppn) %>% sum(./R)
A_gBR <- d.df %>% filter(as.logical(A*B*R)) %>% pull(ppn) %>% sum(./BnR)
B_gAR <- d.df %>% filter(as.logical(A*B*R)) %>% pull(ppn) %>% sum(./AnR)
R_gAB <- d.df %>% filter(as.logical(A*B*R)) %>% pull(ppn) %>% sum(./AnB)
B_gR <- d.df %>% filter(as.logical(B*R)) %>% pull(ppn) %>% sum(./R)
A_gB <- d.df %>% filter(as.logical(A*B)) %>% pull(ppn) %>% sum(./B)
A_gR <- d.df %>% filter(as.logical(A*R)) %>% pull(ppn) %>% sum(./R)
R_gB <- d.df %>% filter(as.logical(B*R)) %>% pull(ppn) %>% sum(./B)
R_gA <- d.df %>% filter(as.logical(A*R)) %>% pull(ppn) %>% sum(./A)
B_gA <- d.df %>% filter(as.logical(B*A)) %>% pull(ppn) %>% sum(./A)
R_gB
R_gAB
A_gB
A_gBR
A_gR
A
A
A_gR
R
R_gA
R_gA/R
A_gR/A
A_gB
A_gBR
x <- 3
+ x
z <- 1
z <- + x
z
1 %/% .2
4 %/% 8
8 %/% 8
8 %/% 4
8 %/% 2
8 %/% 3
setwd("~/ProgramFiles/Github/LifeHistoryStrategies/scripts")
# Load in the packages
for(i in c("MASS", "magrittr", "lubridate", "knitr",
"fitdistrplus", "tidyverse", "kableExtra",
"dplyr")){
library(i, character.only = T)
}
set.seed(123)
# --- Data import -----------
Gdr_loc <- paste0("G:/Shared drives/NASA_ForestScaling/LifeHistoryStrats/",
"data/ForestGEO/processed/example/")
double.df <- read.csv(paste0(Gdr_loc, "wide_HFdata_example.csv"))
head(double.df)
colnames(double.df)
unique(double.df$RecruitStatus)
double.df %>% filter(is.na(RecruitStatus))
double.df %>% filter(is.na(RecruitStatus)) %>% head
dim(double.df)
